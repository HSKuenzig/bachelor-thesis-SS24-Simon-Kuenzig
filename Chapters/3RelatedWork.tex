%*****************************************
\chapter{Related work}\label{ch:relatedWork}
%*****************************************

\section{Large Context Window Models and Their Advancements}\label{sec:large-context}

Recent advances in language model architectures have led to a significant increase in context window sizes, enabling more efficient processing of long and complex input sequences. 
Previous models were restricted to relatively short token limits, such as the 2,048 tokens of \textit{GPT-3}, which limited the possibilities and thus capabilities due to the limited input.
The latest iterations of \ac{sota}\ac{llm}s, such as \textit{Gemini 1.5 Pro} and \textit{GPT-4o}, have expanded these capabilities, achieving context windows of $128,000$ tokens, which would already approximately equal \cite{bb2}, and $1{,}000{,}000$ tokens, respectively. 
%
Large context windows are a desirable feature in \ac{llm} s, but due to, for example, high fine-tuning costs, the scarcity of long text context windows is limited\citep{ding2024longrope}.
Therefore, \citet{ding2024longrope} introduced \ac{longrope}, a \ac{rope} extension capable of scaling context windows beyond two million tokens.
The performance of the models used is maintained in the process, while significantly improving long-text processing efficiency\citep{ding2024longrope}.
\citet{peng2024yarn}, emphasises that \ac{rope} effectively encodes positional information in transformer-based language models to extend the initial smaller context window. 
Due to the remaining limitations of \ac{rope} in generalising past the initial context encodings, \citet{peng2024yarn} demonstrates a compute-efficient adaptation for long-context applications.
Reaching state-of-the-art performances after fine-tuning on less than $\sim0.1\%$ of the original pre-training data significantly reduces the overhead.\citep{peng2024yarn}
The Fine-tuned models that were fine-tuned during the development and testing of \ac{yarn} have been made available online.
By making the technology available as open source material, the context window extension has been reproduced using \ac{yarn} with other language models of individual choice to a context length of $128,000$ token \citep{peng2024yarn}.
This is particularly relevant for \ac{rag} or \ac{qa} setups in resource-constrained environments.
%
Similarly, \citet{zhu2024poseefficientcontextwindow} proposed \ac{pose}, a framework optimised for training long-context models with lower computational overhead. 
\ac{pose}-training decouples train length from target length for efficient context window extension while during the process simulating long inputs using a fixed context window.
Experimental adaptation shows that \ac{pose} significantly reduces overhead compared to full-length fine-tuning\citep{zhu2024poseefficientcontextwindow}.
Therefore, possibly allowing implementation of less capable open-source \ac{llm} s with thus extended context windows in projects with small hardware and time capabilities.
In \citet{kinikogluI2025} the performance of four of the currently most advanced models, \textit{ChatGPT-4o} (San Francisco, CA: OpenAI), \textit{ChatGPT-o1}, Gemini 1.5 Pro (Mountain View, CA: Google LLC), and \textit{Gemini 2.0 Advanced}, were analysed on the Turkish Dental Specialty Examination (DUS) for 2020 and 2021.
All models demonstrated a high level of proficiency in answering the examination question with notable differences in their performance metrics.
%
The \textit{overall accuracy} of \textit{ChatGPT-o1} and \textit{Gemini 2.0 Advanced} remained above $93$ for the exams evaluated outperforming the less advanced models \textit{ChatGPT-4o} and \textit{Gemini 1.5 Pro}.
Further studies of the ability of \ac{llm}s and their potential in medical education and assessment have shown similar results.
%
\citet{USMLE2023, chatgptUSMLE2023} demonstrated that \textit{ChatGPT} achieves passing scores on medical licensing examinations, such as the three US medical licensing examinations (USMLE), even without specific training. 
Therefore, \citet{kinikogluI2025} resumes that such \ac{sota} \ac{llm}s can be adapted to create interactive and personalised learning experiences for students.
%
Despite these improvements, challenges remain in fully leveraging extended context window models. 
Increased computational demands, potential retrieval inefficiencies, and memory constraints must be addressed to ensure effective application in educational and research environments. 

Given these developments, the application of extended context window models in domain-specific \ac{qa} tasks presents an opportunity to enhance accessibility and retrieval accuracy for stakeholders in resource-constrained environments.
Their ability to process entire academic texts directly, without the need for preprocessing or fine-tuning, represents a potential solution to the challenges outlined in \cref{sec:problem statement}.

