%*****************************************
\chapter{Preliminaries}\label{ch:preliminaries}
%*****************************************
This chapter provides the foundational terms and concepts in order to understand the aims and objectives of this thesis.
%
The content is structured as follows: First, an overview of the \ac{snik}-project. Then, \cref{sec:information retrieval} briefly introduces the field of information retrieval. This is followed by \cref{sec:language-models}, providing an insight into language models.
Finally,in \cref{sec:databases} database architectures and the concepts underlying \ac{rag} are discussed. 
%
\\section{Natural Language Processing (NLP) - Introduction}
\begin{definition}
    \ac{nlp} is a subfield of artificial intelligence that addresses the interaction between human language and machines.
    The goal of \ac{nlp} is to enable computer systems to process, interpret, and generate natural language in a meaningful and contextually appropriate way. 
\end{definition}
%
Its importance for current research and application has increased significantly, particularly with the development of powerful language models that can accomplish complex linguistic tasks.
Central to the field are tasks such as question answering, summarisation, named entity recognition, and machine translation, all of which necessitate a nuanced understanding of both syntax and semantics.
The complexity of natural language, which is based on its ambiguity, variability and context dependency, presents computer systems with a variety of challenges.
Words and phrases often have multiple meanings that can only be grasped and resolved by the surrounding text or situational context.
These challenges have led to the development of increasingly sophisticated architectures, such as the transformer model, which has established itself as the basis for current state-of-the-art \ac{nlp} systems due to its scalability and its ability to model contextual relationships more effectively than previous approaches \citep{vaswani2017attention}.
%
\section{Language Models}\label{sec:language-models}
%
\begin{definition}\label{def:language-models}
    \textbf{Language Models} are machine learning models designed to understand and generate human language.
\end{definition}
Language models are built on artificial intelligence techniques and are typically trained using machine learning algorithms to understand and generate human language. 
They have a wide range of applications, including translation, text generation, speech recognition, and text classification.

\subsection{transformer}\label{transformer}
%
\begin{definition}\label{def:transformer}
    A \textbf{Transformer} is a neural network architecture introduced by \citet{vaswani2017attention} that uses attention mechanisms to model dependencies within sequences of text.
\end{definition}

Unlike earlier approaches based on recurrent or convolutional structures, the \textit{Transformer} processes all elements of the input text simultaneously, avoiding the need for sequential computation.
This parallelism, combined with the use of self-attention, allows the model to capture both local and long-range dependencies between textual elements more effectively. 
Due to its scalability and performance, the \textit{Transformer} architecture forms the foundation of many state-of-the-art large language models such as \textit{BERT}, \textit{GPT} and \textit{T5} and are typically trained on extensive data sets to understand statistical patterns within training data.

\section{neural networks}\label{sec:neural-networks}
To understand the architecture of the transformer, it is necessary to know the basics of neural networks.
Neural networks were first described by \citet{neuronal_networks_first} in 1943 and have undergone many changes and improvements since then.
A description of the basic techniques can be found in \citet{neuronale-netze}.
Building on this book, an overview of how neural networks work is given below.

\subsection{Architecture}
Neural networks mimic the functioning of the human brain with the help of mathematical equations.
A network consists of a number of neurons that have both input and output connections to other neurons.
Neurons have the property of sending an impulse from certain inputs to the outputs when a threshold value is exceeded.
In contrast to the brain, however, normal neural networks have a more logical structure.
The neurons are arranged in layers, with the first layer being the input layer with the input neurons $E_N$ and the last layer being the output layer with the output neurons $A_N$.
The layers between these two boundaries are called hidden layers with neurons $H^L_N$.
\todo{Add example}
%
\subsection{Tokens and Tokenisation}
Understanding how natural language is transformed into computationally interpretable representations requires a basic knowledge of tokens and tokenisation.
This section introduces the concept, relevant tokenisation algorithms and explains their importance in the architecture and functionality of \ac{llm}.

\begin{definition}
    In \ac{nlp}, a \textit{token} is a discrete lingustically meaningfull unit of text that serves as input to computational models.

Tokens can represent various linguistic units, including:
\begin{itemize}
  \item \textbf{Full words} (e.g., \textit{hospital})
  \item \textbf{Subwords:} (e.g., \texttt{inter} + \texttt{oper} + \texttt{ability} (interoperability)
  \item \textbf{Individual characters}, especially in character-level models (e.g., \textit{a,e,i,o,!?.-})
\end{itemize}
\end{definition}
%
The way in which text is segmented directly affects the quality and interpretability of the model output because language models process sequences of numerical vectors derived from these tokens.
Tokens, therefore, form the atomic building blocks for \ac{llm} and are essential to enable structured data processing.
%
\subsubsection{The Necessity of Tokenisation}

\begin{definition}
    \textit{Tokenisation} is the process of converting the initial input text into \textit{tokens}.
\end{definition}
%
\ac{lm} process text in the form of \textbf{token}. 
The efficient and structured conversion of potentially huge text input into token via the process of \textit{tokenisation} is therefore essential to control memory and computational requirements.
%
The division of the raw text into \textit{tokens} is done according to a specific tokenisation algorithm.
For example, in the case of the \ac{bpe}-algorithm, frequent character sequences are iteratively merged to form subword tokens.
This allows for efficient handling of out-of-vocabulary (\textit{OOV}), as unfamiliar words can be decomposed into known subwords or, in rare cases, replaced by special unknown tokens, while preserving semantic structure.\\
\textit{Text Chunking $\rightarrow$ textual Tokens}
\begin{itemize}
    \item \textbf{Example (BPE-based tokenisation):}\\
    Compound form - \enquote{\textit{telemonitoringprocess}}
    \begin{quote}
    \texttt{['tele', 'monitor', 'ing', 'process']}
    \end{quote}
\end{itemize}
%
Without \textit{tokenisation}, \ac{llm} would be unable to recognise word boundaries or semantic subcomponents, especially in morphologically rich or compound-heavy languages.
Robust \textit{tokenisation} methods ensure that even rare or unseen terms can be correctly and effectively segmented and interpreted.
%
\subsection{Tokenisation Algorithms}
Modern \ac{llm} rely on data-driven tokenisation techniques to handle vast and diverse vocabularies efficiently. 
Two of the most prominent algorithms are:

\subsubsection{Byte-Pair Encoding (BPE)}
\begin{definition}
    \textbf{Byte-Pair Encoding} is a frequency-based compression algorithm adapted for \ac{nlp} by \citet{sennrich2016neural}. 
\end{definition}

The algorithm begins with creating a character- and byte-level representation of the input text.
Then, the most frequent pairs of adjacent characters and bytes are iteratively merged into new tokens.
The process continues until a predefined vocabulary size is reached, resulting in a compress representation of the original text that balances vocabulary compactness and semantic expressiveness.\\
%
\ac{bpe} is deterministic and scalable, making it widely adopted in \textit{Transformer}-based models such as GPT-2 and GPT-3.
It efficiently reduces the number of unknown tokens while avoiding an unmanageably large vocabulary.

\subsubsection{WordPiece}
\begin{definition}
    \textbf{WordPiece} is a subword tokenisation algorithm developed by \textit{Google}, which selects merges based on likelihood maximisation rather than frequency, and was popularised through its use in \ac{bert} \citep{devlin2019bert}.
\end{definition}
\textit{WordPiece} was originally developed for machine translation tasks at \textit{Google} and later popularised by the \ac{bert} model \citep{devlin2019bert}.
Similarly to \ac{bpe}, \textit{WordPiece} creates subword units, but selects merge operations based on likelihood maximisation rather than simple frequency.
\textit{WordPiece} ensures that commonly occurring word fragments and roots are retained, improving both model generalisation and semantic understanding.

\subsection{Importance of Consistent Tokenisation in \ac{lm}}
Consistent tokenisation is essential to ensure that the input to the \ac{lm} is standardises and can thus be processed reliably and repeatably processed by it.
Since transformer-based models rely on token-level embeddings, inconsistencies in the segmentation from text to tokens can lead to significantly different results, even if the underlying input text remains unchanged.

This is particularly relevant for:
\begin{itemize}
    \item Reproducible benchmarking, as pursued in this thesis
    \item objective model comparison, when evaluating multiple \ac{llm} or configurations
    \item Resource efficiency, since longer token sequences incur greater memory and computational costs
\end{itemize}

In practice, an input that results in 120 tokens for one model may be split into 180 tokens by another model based on differences in tokenisation.
This variation affects latency, inference cost, and model comprehension, especially in length-sensitive architectures with fixed \textit{input windows}.
%

\subsubsection{Predefined Vocabulary and Token-to-ID Mapping}

\begin{definition}
    A \textbf{predefined vocabulary} is a finite list of textual tokens established during the tokeniser's construction, typically before model training.
    It may include full words, frequent subwords, punctuation marks, and domain-specific terminology
\end{definition}

Each token is assigned a unique integer identifier (token ID), enabling the transformation of natural language input into numerical form.
In \ac{lm}, such as \textit{GPT} and \textit{BERT} using static tokenisation algorithms like \textit{BPE} or \textit{Wordpiece}, this vocabulary remains unchanged during inference, ensuring consistency between training and follow-up application.
%
The composition of this vocabulary depends on the applied tokenisation algorithm.
For example, tokens such as \enquote{inter}, \enquote{oper}, and \enquote{ability} may represent common subword components, while \enquote{hospital} and \enquote{systems} could be preserved as individual tokens.
%
Vocabulary sizes typically range between $10,000$ and $50,000$ entries, balancing coverage and computational efficiency.
%
Each \textit{token ID} serves as an index in the model embedding matrix, which stores the corresponding high-dimensional vector representations.
This mapping is foundational for \textit{Transformer}-based architectures, allowing them to convert human language into a mathematically machine-processable format.

\section{Embeddings}\label{sec:embeddings}

\subsection{Encoding}\label{sec:encoding}

\begin{definition}
    \textbf{Encoding} is the conversion of \textit{token} into numerical identifiers.
\end{definition}

During \textbf{Encoding} each textual token is mapped to a unique integer identifier (\textit{ID}) using a predefined vocabulary.
A \textit{predefined vocabulary} consists of a finite list of tokens that are known to the language model and are introduced pre-training.
In this list, which serves as a search table for mapping between textual units and numerical representation, each token is assigned a unique numerical \textit{ID}.\\

Numerical Encoding $\rightarrow$ Token IDs

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{>{\ttfamily}l c >{\ttfamily}l}
\textbf{Token} & $\Rightarrow$ & \textbf{Token ID} \\
\hline
tele        & $\Rightarrow$ & 1021 \\
monitor         & $\Rightarrow$ & 2044 \\
ing      & $\Rightarrow$ & 3090 \\
process           & $\Rightarrow$ & 89 \\
\end{tabular}
\end{center}

These \textit{IDs} serve as input to the model’s \textit{embedding layer}.
In \ac{nlp}, \textit{embeddings} serve as the essential bridge between symbolic representations of language—such as tokens—and the numerical input required by machine learning models.
These vector-based representations encode both semantic and syntactic information, allowing models to capture relationships between words and contextual nuances in language.
In the context of Transformer-based architectures and retrieval-augmented generation (RAG), embeddings are fundamental to understanding meaning, managing context, and enabling attention-based processing over long sequences.

\subsection{Semantic Embeddings}

\begin{definition}\label{def:semantic-embeddings}
\textbf{Semantic embeddings} are dense vector representations of individual \textit{tokens} (e.g., words, subwords) that encode syntactic and semantic relationships based on distributional properties of language~\citep{Mikolov2013EfficientEO, pennington2014glove}.
\end{definition}

\subsubsection{Static Semantic Embeddings}

In earlier neural models, such as \textit{Word2Vec}~\citep{Mikolov2013EfficientEO} and \textit{GloVe}~\citep{pennington2014glove}, \textit{embeddings} are learned by predicting word co-occurrence patterns.
These static \textit{embeddings} assign a single vector to each word, independent of linguistic context, which limits their ability to disambiguate polysemous terms (e.g., \endquote{bank} as a financial institution versus a riverbank).

\subsubsection{Contextual Semantic Embeddings}

\textit{Transformer}-based models overcome this limitation with \textbf{contextual embeddings}, where each \textit{token}'s \textit{embedding} dynamically reflects its meaning based on the surrounding text.
These contextualised vectors are generated dynamically as the model processes input sequences through \textit{self-attention} layers.
For example, in a sentence from medical literature, the \textit{token} \textit{admission} may yield different embeddings when used in the context of \enquote{hospital admission} compared to \enquote{patient admission of symptoms}.

\subsection{Positional Embeddings}

\subsubsection{Necessity of Positional Information}

Unlike recurrent or convolutional models, \textit{Transformer} architectures lack an inherent mechanism to capture the sequential order of tokens. 
Attention mechanisms operate in parallel, all input tokens are processed simultaneously, making it impossible for \ac{lm} to recognise word order unless it is explicitly encoded.
Therefore, \textit{Transformers} require an additional mechanism to inject sequence information into the \ac{lm}’s input.
This is achieved by using \textbf{positional embeddings}.

\begin{definition}
\textit{Positional embeddings} are vector representations added to or concatenated with \textit{token} embeddings to encode the position of a \textit{token} in a sequence, enabling order-aware processing in models based on self-attention~\citep{vaswani2017attention}.
\end{definition}

\subsubsection{Types of Positional Embeddings}

There are two primary types of \textit{positional embeddings}:
\begin{itemize}
    \item \textbf{Absolute positional embeddings:} Each position in the sequence is assigned a unique, fixed vector, often derived from sinusoidal functions as introduced by \citet{vaswani2017attention}.
    \item \textbf{Relative positional embeddings:} These embeddings encode the relative distance between \textit{tokens}, rather than their absolute positions. This formulation has been shown to improve generalisation across sequence lengths~\citep{shaw2018self, dai2019transformer}.
\end{itemize}

Relative positional representations allow the \ac{lm} to focus on inter-token relations and dependencies without being limited by fixed input lengths, enhancing traditional \textit{Transformers} by generalising to longer or variable-length inputs, which makes them particularly useful in \ac{lm} designed for long-context retrieval.

\subsection{Embeddings in Large-Context Models and RAG Systems}

In large-context models, such as \textit{GPT-4} or \textit{Claude} and in \ac{rag} systems, \textit{embeddings} are essential to manage and attend to extended sequences as it is the fundamental mechanism through which semantic meaning and positional context are encoded.
It allows \ac{lm} to process and reason over large inputs and their content with coherence and relevance.
These \ac{lm} rely on both \textit{token} and \textit{positional embeddings} to maintain and ensure semantic precision and provide contextual relevance over potentially thousands of tokens.

In \ac{rag}, embeddings are also used for information retrieval, where a query embedding is compared with pre-computed document embeddings, for example, when the relevant document is stored in a \textit{vector database}, using techniques such as vector similarity search to select the most relevant context for generation.
The quality and structure of these embeddings have a direct impact on the retrieval accuracy and the quality of the subsequent generation~\citep{Lewis2020}.
%
\section{Sequence Input and Chunking}

\subsection{Context Windows and Maximum Token Limits}

\textit{Transformer}-based \ac{llm} operate on fixed-length \textit{token} sequences, which are constrained by a predefined \textit{context window}.

\begin{definition}
A \textbf{context window} refers to the maximum number of input \textit{tokens} a \textit{Transformer} model can attend to at any point in time.
In autoregressive models, this includes both the input prompt and all tokens generated during the inference process.
\end{definition}

The \textit{BERT-base} model supports $512$ \textit{tokens}~\citep{devlin2019bert}, OpenAI's older \textit{GPT-3} is limited to $2048$ \textit{tokens}~\citep{gpt3} during a single inference pass.
Current models, such as \textit{GPT-4} and Anthropics \textit{Claude 3} are capable of handling $32,000$ to $200,000$ tokens~\citep{gpt4, anthropic2024claude3} allowing for significantly larger \textit{context windows}.
Despite such advances, limitations persist due to computational complexity, memory requirements, and model degradation at extreme lengths~\citep{liu2024lostmiddle}.

\subsection{The Need for Chunking}

The size of the context windows has been constantly extended through development iterations.
Nevertheless, the demand for even larger inputs does not match the available options.
Therefore, if the input sequences exceed the context window, the input must be modified.
\begin{definition}
\textbf{Chunking} is the process of dividing longer textual sequences into smaller, contextually meaningful segments (\textit{chunks}) that fit within a \ac{lm}'s context window.
\end{definition}
The quality and effectiveness of the \textit{chunking} algorithms has a significant impact on the information and knowledge extraction, as well as on the generated output of the respective language model.
Depending of the chunking process truncated information, loss of context, or semantic drift—particularly may occur and are particualry problematic in high-stakes domains such as medical texts.

\subsection{Chunking Techniques}

Several chunking strategies are used to structure input sequences appropriately:

\begin{itemize}
    \item \textbf{Fixed-size chunking:} Text is divided into equal-sized chunks (e.g., 512 tokens).
    This approach is simple but may split sentences or logical units, therefore disrupting and loosing the coherent contextual meaning.
    
    \item \textbf{Sliding window:} Overlapping chunks are created by shifting a fixed window across the input with some adaptable overlap (e.g., $512$-token chunks with $128$-token overlap).
    In this process, each token attends to a fixed-size window of tokens around it and with that stacking multiple layers of overlapping attention allows for a large receptive field.
    This approach helps maintain contextual continuity across boundaries~\citep{beltagy2020longformerlongdocumenttransformer}.
    \item \textbf{Sentence- or paragraph-based chunking:} Input is segmented based on natural linguistic units, preserving semantic integrity.
    Although more coherent, by creating chunks with inconsistent size due to varying sentence- and paragraph length, this method may lead to uneven token distribution and inefficient memory use.
\end{itemize}

\subsection{Trade-offs in Context Length and Performance}

There exists a trade-off between longer context windows and model performance.
Longer sequences allow more complete and coherent context.
At the same time, they come with increased memory usage, computational cost, and potential attention diffusion~\citep{liu2024lostmiddle}.
Models may struggle to retain or prioritise relevant information across extended inputs, especially without mechanisms such as \textit{recurrence} or \textit{global attention}.

\subsection{Recurrence and Global Attention in Transformer Architectures}

Transformer-based models process sequences in parallel, which makes them efficient to compute and process, but poses challenges in preserving dependencies and context over long distances, especially in expanded input contexts.
To address this limitation, architectural extensions such as \textit{recurrence} and \textit{global attention} have been proposed to improve information storage and prioritisation.

\subsubsection{Recurrence}

\begin{definition} \textbf{Recurrence} in Transformers refers to a mechanism where representations from previous input segments (e.g., past chunks or text sequences) are recurrently fed into the \ac{lm} to preserve historical context.
\end{definition}

This technique allows the model to incorporate information from already processed segments without reprocessing the entire sequence.
\citet{dai2019transformerxl} introduced segment-level recurrence by caching hidden states from previous segments, enabling learning across longer contexts without compromising computational efficiency.
This recurrent memory leads to better modelling of temporal dependencies and improves performance in long-form text generation and reasoning.

\subsubsection{Global Attention}

\begin{definition} \textbf{Global attention} is a modification of the self-attention mechanism that designates a subset of tokens, predominantly structurally or semantically important \textit{token} in form a internal representation, to attend to all other tokens in the input sequence, while also allowing all \textit{tokens} to attend to these designated internal representations.
\end{definition}


\section{Information Retrieval}\label{sec:information retrieval}

\begin{definition}\label{def:information retrieval}
Information retrieval is the process of identifying, retrieving and ranking relevant information from resources in response to a previous query.
\end{definition}
%
The traditional \ac{ir} was based on a single-stage term-based system, which was first applied in scientific research.
The matching phrases and terms from the source were searched according to the search term and generated as direct output.
However, the quality of the output suffered from simple linguistic hurdles, e.g., polysemy, synonymy, and lexical gaps, and was therefore limited in their effectiveness. 
The addition of ranking mechanisms led to more accurate results, as ranking is carried out in a second stage. 
The matching terms and retrieved documents are pre-sorted by ranking algorithms, e.g., BM25, on the basis of a relevance score. 
Over time, advanced ranking and sorting algorithms have been integrated to further improve the results. \citep{hambardeIR2023}
Starting with simple text retrieval, multi-modal retrieval is now possible.
%

\subsection{Question Answering}\label{subsec:question answering}

\begin{definition}
\ac{qa} is an area within \ac{nlp} and \ac{ir} that focusses on the development of systems in which questions posed by the user in natural language are to be answered. 
The entered question is first analysed to extract the intention of the query. 
Based on this, relevant information is retrieved from a source,  further processed using \ac{ir} technologies and then combined into a coherent answer.\citep{Manning2008}.
\end{definition}
%
\section{Databases}\label{sec:databases}
\begin{definition}\label{def:database}
    A \textbf{database} is a collection of related data\citep{limited2010introduction}. 
    It is used and designed to efficiently handle data storage, retrieval, and management on computer systems. 
    It contains and arranges data in a manner that makes it accessible by a variety of applications.
\end{definition}
Databases are essential tools for the storage and management of large volumes of information. 
Their structures vary significantly, with relational databases that organise data in table formats, while \ac{nosql} databases offer more adaptable frameworks suited to diverse data types. 
The way data are stored is influenced by both the specific demands of the application and the architectural design of the chosen database model.
\section{Database Management System}\label{sec:Database Management System}
\begin{definition}\label{def:Database Management System}
    A \ac{dbms}  is a software system designed to create a database and subsequently execute operations that allow the definition, manipulation, and efficient retrieval of the stored data. 
    At the same time, database security and potential recovery must be ensured during a failure or unforeseen process sequence. \citep{limited2010introduction}
\end{definition}
A database and \ac{dbms} together are defined as a database system.  

\subsection{Database Architectures}\label{sec:database-architectures}
Databases can be classified into different architectures, including relational and \ac{nosql} databases. 
Relational databases use a table-based structure with a predefined schema. 
They are widely used for applications that require consistency and follow a clear generic pattern, such as \ac{sql}-based systems.
\ac{nosql} databases offer more flexibility and can store unstructured data. 
For example, graph databases are designed to efficiently model and navigate complex networks of interconnected data, making them ideal for applications that require understanding relationships, such as social networks, recommendation systems, and knowledge graphs.
\subsubsection{Unstructured data}\label{sec:unstructured-data}
\begin{definition}\label{def:unstructured-data}
    \textbf{Unstructured data} refers to information that does not have a predefined data model or organised structure, making it more challenging to store, search and analyze using traditional database systems.
\end{definition}
Unstructured data includes formats such as text documents, images, audio, and video files, where the data is not organised in a tabular or relational structure. 
Due to its lack of a fixed schema, unstructured data often requires specialised processing techniques, such as \ac{nlp} or machine learning, to extract meaningful information and be able to process those further.
\section{Databases for Retrieval-Augmented Generation}\label{sec:databases-rag}

\subsection{Relational Databases}\label{sec:relational-databases}
Relational databases organise data into tables with predefined\\ schemas, allowing for structured data storage. 

\subsection{NoSQL Databases}\label{sec:nosql-databases}
\ac{nosql} databases provide a more flexible approach to data storage, supporting various data structures such as documents, key-value pairs, and graphs. 
They are particularly suitable for handling unstructured or semi-structured data and are frequently used in \ac{rag} implementations for efficient storage and retrieval.

\subsection{Document Stores}\label{sec:document-stores}
Document-oriented databases, like MongoDB and Elasticsearch, are optimised for storing and querying large amounts of unstructured text data. 
These databases allow efficient indexing and retrieval of documents, making them highly suitable for \ac{rag}, where retrieving relevant text passages is a priority.
\subsection{Graph Databases}\label{sec:graph-databases}
Graph databases are designed to store and manage data as a collection of nodes, representing entities, and edges, which capture the relationships between these entities \citep{Robinson2015}. This structure enables efficient handling of highly interconnected data, making graph databases particularly suitable for applications such as social networks, recommendation engines, and knowledge graphs \citep{Angles2008}. In the context of \ac{rag}, graph databases support the retrieval of contextually relevant information by leveraging the natural relationships embedded within the data, thus improving the coherence and accuracy of generated responses. Prominent graph database systems include Neo4j and Amazon Neptune, which are widely used for their scalability and performance in handling large-scale graph-based queries.

\subsection{Vector Databases}\label{sec:vector-databases}
Vector databases are specialised systems designed to store and manage data as high-dimensional vectors, facilitating efficient similarity searches by measuring distances between these vectors. 
This capability is crucial in applications like \ac{rag}, where embedding-based retrieval methods are used to quickly access semantically relevant documents. 
Notable examples of vector databases include Pinecone and FAISS, both optimised for large-scale real-time vector retrieval tasks \citep{Johnson2019}.

\subsection{Retrieval Methods}\label{sec:retrieval-methods}

Various retrieval methods are used to extract data or information from a data source, such as databases. Depending on the database architecture and the level of detail of the query, different methods are used, including keyword search, Boolean search, and vector-based similarity search. Each method is suitable for specific data types and query requirements.
\section{Concepts for Data Extraction}\label{sec:data-extraction-concepts}

\subsection{Keyword-Based Search}\label{sec:keyword-search}
Keyword-based search is a fundamental retrieval method that retrieves documents or records containing specific keywords. It offers a simple approach but is often less effective for complex queries where semantic understanding of context is required.

\subsection{Vector-Based Similarity Search}\label{sec:vector-similarity-search}
Vector-based similarity search retrieves relevant data by comparing text vector embeddings. Embeddings represent the semantic meaning of text, allowing for more accurate retrieval in cases where traditional keyword searches provide inaccurate results.

\subsection{Neural Retrieval Methods}\label{sec:neural-retrieval}
Neural retrieval models, such as \ac{dpr}, use deep learning to learn vector embeddings for queries and documents. These embeddings are used for high-accuracy retrieval, where the model ranks documents based on their semantic relevance to a given query.

\section{Underlying Aim of Retrieval-Augmented Generation}\label{sec:aim-of-rag}

\begin{definition}\label{def:rag}
    \textbf{\ac{rag}} is a technique that integrates information retrieval with language generation, allowing models to produce responses grounded in relevant external data, thus enhancing the factual accuracy and contextual relevance of generated content.
\end{definition}

The primary objective of \ac{rag} is to combine the generative capabilities of \ac{llm}'s with the precision of retrieval systems. 
By accessing external knowledge bases, a \ac{rag}-system retrieves pertinent information to inform and refine generated responses, ensuring that they are both contextually appropriate and factually correct \citep{Lewis2020}.


\subsection{Combining Retrieval and Generation}\label{sec:combining-retrieval-generation}
In \ac{rag}, the system first retrieves relevant documents or passages from a database and then uses a generative language model, such as \ac{llm}'s, to create responses based on this content. This approach allows \ac{rag} to produce answers that are contextually relevant and factually accurate.


\section{Medical Informatics}\label{sec:medical-informatics}


Da sich die Medizinische Informatik mit der Lösung medizinischer Probleme befasst, sollen hier auch die Hintergründe des medizinischen Problems so dargestellt und erläutert werden, dass sie auch für Leser der Arbeit, die nicht Mediziner sind, verständlich sind.

In diesem Kapitel werden auch die Methoden erläutert, die zur Lösung der Probleme eingesetzt wurden.
Stellen Sie sicher, dass hier alle, aber auch nur die Grundlagen und Methoden erläutert werden, die in der Arbeit verwendet wurden.
Stellen Sie im weiteren Text der Arbeit auch sicher, dass der Leser erkennen kann, wie Sie unter Verwendung der Methoden zu Ihren Ergebnissen gekommen sind.
So sollten z.B. Modellierungsmethoden nur verwendet werden, wenn die Modelle nachvollziehbar dazu genutzt werden, die Ergebnisse zu erzielen.
